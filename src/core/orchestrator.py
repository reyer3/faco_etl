"""
ETL Orchestrator for FACO ETL

Real implementation using the complete ETL pipeline modules.
"""

from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional, Dict
from loguru import logger
import pandas as pd

from .config import ETLConfig


@dataclass
class ETLResult:
    """ETL execution result"""
    success: bool
    records_processed: int
    execution_time: str
    output_tables: List[str]
    data_quality_report: Optional[Dict] = None
    error_message: Optional[str] = None


class ETLOrchestrator:
    """Complete orchestrator that coordinates the real ETL pipeline"""
    
    def __init__(self, config: ETLConfig):
        self.config = config
        
        # Initialize ETL components - lazy loading to avoid import errors in mock mode
        self._extractor = None
        self._business_days = None
        self._transformer = None
        self._loader = None
        
        logger.info("üèóÔ∏è  ETL Orchestrator inicializado")
        
    def _initialize_components(self):
        """Initialize ETL components with lazy loading"""
        if self._extractor is None:
            try:
                from etl.extractor import BigQueryExtractor
                from etl.business_days import BusinessDaysProcessor
                from etl.transformer import CobranzaTransformer
                from etl.loader import BigQueryLoader
                
                self._extractor = BigQueryExtractor(self.config)
                self._business_days = BusinessDaysProcessor(self.config)
                self._transformer = CobranzaTransformer(self.config, self._business_days)
                self._loader = BigQueryLoader(self.config)
                
                logger.info("‚úÖ Componentes ETL reales inicializados")
                
            except ImportError as e:
                logger.warning(f"‚ö†Ô∏è  No se pudieron importar m√≥dulos ETL reales: {e}")
                logger.warning("üîÑ Usando modo mock para testing")
                return False
        return True
        
    def run(self) -> ETLResult:
        """Run the complete ETL pipeline"""
        start_time = datetime.now()
        
        try:
            logger.info("üöÄ Iniciando pipeline ETL")
            
            # Validate configuration
            self.config.validate()
            logger.info(f"‚úÖ Configuraci√≥n validada - Proyecto: {self.config.project_id}")
            
            # Try to initialize real components
            if self._initialize_components():
                return self._run_real_etl(start_time)
            else:
                return self._run_mock_etl(start_time)
                
        except Exception as e:
            execution_time = str(datetime.now() - start_time)
            logger.error(f"üí• Error en ETL Pipeline: {e}")
            
            return ETLResult(
                success=False,
                records_processed=0,
                execution_time=execution_time,
                output_tables=[],
                error_message=str(e)
            )
    
    def _run_real_etl(self, start_time: datetime) -> ETLResult:
        """Run the real ETL pipeline with actual data processing"""
        logger.info("üéØ Ejecutando pipeline ETL real")
        
        # Step 1: Test connectivity
        logger.info("üì° Paso 1: Verificando conectividad")
        if not self._extractor.test_connectivity():
            raise ValueError("Error de conectividad con BigQuery")
        
        # Step 2: Extract data from BigQuery
        logger.info("üì• Paso 2: Extrayendo datos de BigQuery")
        raw_data = self._extractor.extract_all_data()
        
        if not raw_data or all(df.empty for df in raw_data.values()):
            raise ValueError("No se encontraron datos para procesar")
        
        extract_summary = {table: len(df) for table, df in raw_data.items()}
        logger.info(f"‚úÖ Extracci√≥n completada: {extract_summary}")
        
        # Step 3: Transform data with business logic
        logger.info("üîÑ Paso 3: Transformando datos con l√≥gica de negocio")
        transformed_data = self._transformer.transform_all_data(raw_data)
        
        if not transformed_data or all(df.empty for df in transformed_data.values()):
            raise ValueError("La transformaci√≥n no gener√≥ datos v√°lidos")
        
        transform_summary = {table: len(df) for table, df in transformed_data.items()}
        logger.info(f"‚úÖ Transformaci√≥n completada: {transform_summary}")
        
        # Step 4: Validate data quality
        logger.info("üîç Paso 4: Validando calidad de datos")
        quality_report = self._loader.validate_data_quality(transformed_data)
        
        # Check for critical quality issues
        critical_issues = []
        for table, report in quality_report.items():
            if report.get("status") == "FAIL":
                critical_issues.append(f"Tabla {table} fall√≥ validaci√≥n de calidad")
        
        if critical_issues and not self.config.dry_run:
            raise ValueError(f"Problemas cr√≠ticos de calidad: {', '.join(critical_issues)}")
        
        logger.info("‚úÖ Validaci√≥n de calidad completada")
        
        # Step 5: Load data to BigQuery
        logger.info("üíæ Paso 5: Cargando datos a BigQuery")
        load_results = self._loader.load_all_tables(transformed_data)
        
        # Step 6: Apply Looker Studio optimizations
        if not self.config.dry_run:
            logger.info("‚ö° Paso 6: Aplicando optimizaciones para Looker Studio")
            self._loader.create_table_descriptions()
            self._loader.optimize_for_looker_studio()
        
        # Calculate execution metrics
        execution_time = str(datetime.now() - start_time)
        total_records = sum(len(df) for df in transformed_data.values())
        successful_tables = [table for table, result in load_results.items() 
                           if result.get('status') in ['SUCCESS', 'DRY_RUN']]
        
        # Prepare success result
        result = ETLResult(
            success=True,
            records_processed=total_records,
            execution_time=execution_time,
            output_tables=successful_tables,
            data_quality_report=quality_report
        )
        
        # Log success summary
        logger.success("üéâ ETL Pipeline real completado exitosamente")
        logger.info(f"üìä Registros procesados: {total_records:,}")
        logger.info(f"‚è±Ô∏è  Tiempo total: {execution_time}")
        logger.info(f"üìã Tablas generadas: {', '.join(successful_tables)}")
        
        # Log quality summary
        for table, report in quality_report.items():
            status = report.get("status", "UNKNOWN")
            row_count = report.get("row_count", 0)
            issues = report.get("issues", [])
            
            if status == "PASS":
                logger.info(f"‚úÖ {table}: {row_count:,} registros - Sin problemas")
            elif status == "WARNING":
                logger.warning(f"‚ö†Ô∏è  {table}: {row_count:,} registros - {len(issues)} advertencias")
            elif status == "EMPTY":
                logger.warning(f"üì≠ {table}: Tabla vac√≠a")
            elif status == "FAIL":
                logger.error(f"‚ùå {table}: {row_count:,} registros - FALL√ì validaci√≥n")
        
        return result
    
    def _run_mock_etl(self, start_time: datetime) -> ETLResult:
        """Run mock ETL process for testing when real modules aren't available"""
        logger.info("üé≠ Ejecutando pipeline ETL mock")
        
        # Mock ETL process
        records_processed = self._mock_etl_process()
        
        execution_time = str(datetime.now() - start_time)
        
        return ETLResult(
            success=True,
            records_processed=records_processed,
            execution_time=execution_time,
            output_tables=list(self.config.output_tables.values())
        )
    
    def _mock_etl_process(self) -> int:
        """Mock ETL process for initial testing"""
        import time
        
        logger.info(f"üìÖ Procesando mes: {self.config.mes_vigencia}")
        logger.info(f"üìä Estado vigencia: {self.config.estado_vigencia}")
        
        if self.config.dry_run:
            logger.warning("üèÉ‚Äç‚ôÇÔ∏è Modo DRY-RUN activado - No se escribir√° a BigQuery")
        
        # Simulate processing steps
        steps = [
            "üì• Extrayendo datos del calendario",
            "üîÑ Procesando dimensiones de asignaci√≥n", 
            "ü§ñ Agregando m√©tricas de gesti√≥n BOT",
            "üë®‚Äçüíº Agregando m√©tricas de gesti√≥n HUMANA",
            "üìä Calculando KPIs y m√©tricas comparativas",
            "üìÖ Procesando d√≠as h√°biles",
            "üîç Validando calidad de datos"
        ]
        
        for step in steps:
            logger.info(step + "...")
            time.sleep(0.8)  # Simulate processing time
        
        if not self.config.dry_run:
            logger.info("üíæ Cargando tablas agregadas a BigQuery...")
            time.sleep(1)
        
        # Mock processed records
        return 42000
    
    def validate_connectivity(self) -> Dict[str, bool]:
        """Validate connectivity to required services"""
        logger.info("üîó Validando conectividad")
        
        if not self._initialize_components():
            return {"components_initialized": False}
        
        connectivity_results = {}
        
        # Test BigQuery connectivity
        try:
            if self._extractor.test_connectivity():
                connectivity_results["bigquery"] = True
                logger.info("‚úÖ BigQuery: Conectado")
            else:
                connectivity_results["bigquery"] = False
                logger.error("‚ùå BigQuery: Error de conexi√≥n")
        except Exception as e:
            connectivity_results["bigquery"] = False
            logger.error(f"‚ùå BigQuery: Error de conexi√≥n - {e}")
        
        # Test business days logic
        try:
            test_date = datetime.now().date()
            validation_results = self._business_days.validate_business_day_logic()
            connectivity_results["business_days"] = all(validation_results.values())
            
            if connectivity_results["business_days"]:
                logger.info("‚úÖ D√≠as h√°biles: Funcionando correctamente")
            else:
                logger.error("‚ùå D√≠as h√°biles: Error en validaci√≥n")
        except Exception as e:
            connectivity_results["business_days"] = False
            logger.error(f"‚ùå D√≠as h√°biles: Error - {e}")
        
        return connectivity_results
    
    def run_data_preview(self, limit: int = 100) -> Dict[str, pd.DataFrame]:
        """Run a limited preview of the ETL process for testing"""
        logger.info(f"üëÄ Ejecutando preview de datos (l√≠mite: {limit})")
        
        if not self._initialize_components():
            logger.warning("‚ö†Ô∏è  No se pueden ejecutar previews sin componentes reales")
            return {}
        
        try:
            # Temporarily reduce batch size for preview
            original_batch_size = self.config.batch_size
            self.config.batch_size = min(limit, original_batch_size)
            
            # Extract limited data
            raw_data = self._extractor.extract_all_data()
            
            # Limit each DataFrame
            limited_data = {}
            for table, df in raw_data.items():
                if not df.empty:
                    limited_data[table] = df.head(limit)
                else:
                    limited_data[table] = df
            
            # Transform limited data
            preview_data = self._transformer.transform_all_data(limited_data)
            
            # Restore original batch size
            self.config.batch_size = original_batch_size
            
            # Log preview summary
            for table, df in preview_data.items():
                logger.info(f"üìã Preview {table}: {len(df)} registros, {len(df.columns)} columnas")
            
            return preview_data
            
        except Exception as e:
            logger.error(f"‚ùå Error en preview: {e}")
            return {}
    
    def get_processing_summary(self) -> Dict:
        """Get summary of what will be processed"""
        if not self._initialize_components():
            return {"error": "No se pudieron inicializar componentes ETL"}
        
        try:
            # Get calendar data to understand scope
            calendario_query = f"""
            SELECT 
                COUNT(*) as total_periodos,
                MIN(FECHA_ASIGNACION) as fecha_inicio,
                MAX(FECHA_CIERRE) as fecha_fin,
                STRING_AGG(DISTINCT ESTADO) as estados,
                COUNT(DISTINCT ARCHIVO) as archivos_unicos
            FROM `{self.config.project_id}.{self.config.dataset_id}.dash_P3fV4dWNeMkN5RJMhV8e_calendario_v3`
            WHERE DATE_TRUNC(FECHA_ASIGNACION, MONTH) = DATE('{self.config.mes_vigencia}-01')
            AND UPPER(ESTADO) = UPPER('{self.config.estado_vigencia}')
            """
            
            calendar_summary = self._extractor.client.query(calendario_query).to_dataframe()
            
            if calendar_summary.empty:
                return {"error": "No se encontraron per√≠odos para procesar"}
            
            row = calendar_summary.iloc[0]
            
            return {
                "mes_vigencia": self.config.mes_vigencia,
                "estado_vigencia": self.config.estado_vigencia,
                "total_periodos": int(row['total_periodos']),
                "fecha_inicio": row['fecha_inicio'],
                "fecha_fin": row['fecha_fin'],
                "estados": row['estados'],
                "archivos_unicos": int(row['archivos_unicos']),
                "configuracion": {
                    "batch_size": self.config.batch_size,
                    "max_workers": self.config.max_workers,
                    "include_saturdays": self.config.include_saturdays,
                    "dry_run": self.config.dry_run
                }
            }
            
        except Exception as e:
            logger.error(f"Error obteniendo resumen de procesamiento: {e}")
            return {"error": str(e)}